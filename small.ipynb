{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten(\n",
    "    ) if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten(\n",
    "    ) if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten(\n",
    "    ) if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten(\n",
    "    ) if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mediapipe\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Function to extract keypoints from a frame\n",
    "\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = model.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mediapipe' has no attribute 'mediapipe_detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=32'>33</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=34'>35</a>\u001b[0m \u001b[39m# Make detections\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=35'>36</a>\u001b[0m image, results \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39;49mmediapipe_detection(frame, holistic)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=37'>38</a>\u001b[0m draw_styled_landmarks(image, results)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=39'>40</a>\u001b[0m \u001b[39m# Extract keypoints\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mediapipe' has no attribute 'mediapipe_detection'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the trained model\n",
    "final_model = load_model('slt_model.h5')\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.joblib')\n",
    "\n",
    "# Loading file\n",
    "int_to_label = np.load('label_to_int.npy', allow_pickle=True).item()\n",
    "\n",
    "# Open a connection to the webcam (change the index if you have multiple cameras)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize Mediapipe\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Set the maximum sequence length and feature dimensions\n",
    "max_length = 195\n",
    "feature_dimensions = 1662\n",
    "\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame\")\n",
    "            break\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mp.mediapipe_detection(frame, holistic)\n",
    "\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Extract keypoints\n",
    "        keypoints = extract_keypoints(results)\n",
    "\n",
    "        print(keypoints)\n",
    "\n",
    "        new_data_array = keypoints\n",
    "\n",
    "        # Add batch dimension\n",
    "        new_data_point = np.expand_dims(new_data_array, axis=0)\n",
    "\n",
    "        # Reshape to 2D array (flatten the additional dimensions)\n",
    "        new_data_point_2d = new_data_point.reshape(\n",
    "            -1, new_data_point.shape[-1])\n",
    "\n",
    "        # Use the scaler to transform the data\n",
    "        new_data_scaled = scaler.transform(new_data_point_2d)\n",
    "\n",
    "        # Ensure consistency in the number of features\n",
    "        if len(keypoints) != feature_dimensions:\n",
    "            print(\n",
    "                \"Number of features in the new data does not match the expected number.\")\n",
    "            continue\n",
    "\n",
    "        new_data_padded = pad_sequences(\n",
    "            [new_data_scaled], padding='post', maxlen=max_length, dtype='float32')\n",
    "\n",
    "        # Verify the shape of new_data_padded\n",
    "        print(\"Shape of new_data_padded:\", new_data_padded.shape)\n",
    "\n",
    "        # Make predictions using the trained model\n",
    "        predictions = final_model.predict(new_data_padded)\n",
    "\n",
    "        # Post-process the predictions (e.g., convert probabilities to labels)\n",
    "        predicted_label = np.argmax(predictions)\n",
    "\n",
    "        # Decode the integer label to the original label\n",
    "        predicted_sign = int_to_label[predicted_label]\n",
    "\n",
    "        print(predicted_sign)\n",
    "\n",
    "        # Display the predicted sign on the frame\n",
    "        cv2.putText(frame, f\"Prediction: {predicted_sign}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Real-Time SLT Prediction', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture object and close the OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
